---
layout: none
permalink: /blog/index.html/
---

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <link href="https://fonts.googleapis.com/css?family=Roboto:400,700" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:400,700|Merriweather:400,700" rel="stylesheet">



    <title>Liquid Neural Networks - A Modern Approach</title>
    
    <script type="text/javascript" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>

    <style>
        *, *::before, *::after {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        table {
    border-collapse: collapse;
    width: 100%;
    font-family: Arial, sans-serif;
    margin: 20px 0;
  }

  th, td {
    border: 1px solid #ddd;
    padding: 8px;
    text-align: left;
  }

  th {
    background-color: #f4f4f4;
    color: #333;
    font-weight: bold;
  }

  tr:nth-child(even) {
    background-color: #f9f9f9;
  }

  tr:hover {
    background-color: #f1f1f1;
  }

  .serial-column {
    text-align: center;
    width: 5%;
  }

  caption {
    font-weight: bold;
    font-size: 1.2em;
    margin-bottom: 10px;
  }

        body {
            font-family: 'Open Sans', sans-serif; /* body text */
            line-height: 1.6;
            color: #333;
        }

        h1, h2, h3 {
            font-family: 'Merriweather', serif; /* headings */
        }

        .citation {
            color: blue;
            text-decoration: none;
        }

        header {
            background: linear-gradient(to right, #4f5bd5, #962fbf, #d62976, #fa7e1e, #feda75);
            padding: 40px 20px;
            position: sticky;
            top: 0;
            z-index: 999;
            transition: all 0.3s ease;
            display: flex;
            flex-direction: column;
            align-items: center;
            flex-wrap: nowrap; /* Prevent wrapping */
        }

        header h1 {
            font-size: 3rem;
            margin-bottom: 10px;
            font-weight: 700;
            color: #fff;
            transition: font-size 0.3s ease, margin-bottom 0.3s ease;
            text-align: center;
            white-space: nowrap; /* Prevent text from wrapping */
        }

        header p {
            font-size: 1.2rem;
            margin-bottom: 0;
            color: #fff;
            transition: opacity 0.3s ease, height 0.3s ease;
        }

        nav {
            opacity: 0;
            visibility: hidden;
            transition: opacity 0.3s ease, visibility 0.3s ease;
            white-space: nowrap; /* Ensure nav items stay in one line */
        }

        nav ul {
            display: flex;
            list-style: none;
            padding: 10px;
        }

        nav li {
            margin: 0 15px;
        }

        nav a {
            text-decoration: none;
            color: #fff;
            font-weight: 500;
            transition: color 0.3s ease;
        }

        nav a:hover {
            color: #333;
        }

        .header-scrolled {
            padding: 10px 20px;
            flex-direction: row;
            justify-content: space-between;
            align-items: center;
        }

        .header-scrolled h1 {
            font-size: 1.8rem;
            margin-bottom: 0;
            text-align: left;
        }

        .header-scrolled p {
            opacity: 0;
            height: 0;
            overflow: hidden;
        }

        .header-scrolled nav {
            opacity: 1;
            visibility: visible;
        }

        main {
            max-width: 800px;
            margin: 40px auto;
            padding: 0 20px;
        }

        main h2 {
            font-size: 2.5rem;
            margin-bottom: 20px;
            border-bottom: 2px solid #eee;
            padding-bottom: 10px;
        }

        main h3 {
            font-size: 1.8rem;
            margin-top: 30px;
            margin-bottom: 15px;
        }

        main p {
            margin-bottom: 15px;
            font-size: 1.1rem;
        }

        

        .image-container {
            display: flex;
            flex-wrap: wrap;
            gap: 20px;
            margin-bottom: 30px;
        }

        .image-container img {
            width: 100%;
            max-width: 350px;
            border-radius: 5px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }

        figure {
            text-align: center;
            margin: 20px 0;
        }

        figure img {
            max-width: 100%;
            height: auto;
            border-radius: 5px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }

        figure figcaption {
            font-size: 0.9rem;
            color: #555;
            margin-top: 5px;
        }

        blockquote {
            border-left: 4px solid #962fbf;
            padding-left: 15px;
            font-style: italic;
            margin: 20px 0;
            color: #555;
        }

        code, pre {
            background: #f2f2f2;
            padding: 5px;
            border-radius: 3px;
            font-family: Consolas, "Courier New", monospace;
        }

        footer {
            display: flex;
            justify-content: center;
            align-items: center;
            padding: 20px;
            font-size: 0.9rem;
            color: #666;
            background: #fff;
            border-top: 1px solid #ddd;
            margin-top: 40px;
        }

        .word-count {
            font-size: 0.9rem;
            color: #666;
        }

        pre {
            background-color: #f9f9f9;
            border: 1px solid #ddd;
            padding: 10px;
            overflow-x: auto;
            margin: 20px 0;
        }

        .equation {
            display: flex;
            justify-content: center;
            align-items: center;
            margin: 20px 0;
            position: relative;
        }
        .equation span.equation-number {
            position: absolute;
            right: 0;
            font-weight: bold;
        }

        ul {
            margin: 10px 0;
            padding-left: 20px;
        }

        li {
            margin: 5px 0;
        }
        .relationships {
            margin-top: 20px;
            padding: 15px;
            border: 1px solid #bdc3c7;
            background-color: #ecf0f1;
            border-radius: 5px;
        }

        .latex {
            background-color: #f8f9fa;
            border: 1px solid #ced4da;
            padding: 10px;
            border-radius: 5px;
            font-family: "Courier New", Courier, monospace;
        }

        /* Ensure headings are visible beneath the sticky header when navigated to */
        section[id], 
        [id] {
            scroll-margin-top: 100px; /* Adjust this based on header height */
        }

        @media (max-width: 600px) {
            header h1 {
                font-size: 2rem;
            }

            .header-scrolled h1 {
                font-size: 1.5rem;
            }

            nav ul {
                flex-direction: row; /* Keep items in one line as requested */
            }

            section[id] {
                scroll-margin-top: 120px; /* Slightly larger for smaller screens if needed */
            }

            footer {
                flex-direction: column;
                align-items: flex-start;
            }
        }
    </style>
</head>

<body>
    <header id="page-header">
        <h1>Liquid Neural Networks</h1>
        <!--<p>An Exploration of Dynamic, Adaptive Neural Architectures</p>-->
        <p>The one-stop blog to understand their inner workings</p>
        
        <nav>
            <ul>
                <li><a href="#introduction">Introduction</a></li>
                <li><a href="#background">Background</a></li>
                <li><a href="#methods">Methods</a></li>
                <li><a href="#experiments">Experiments</a></li>
                <li><a href="#discussion">Discussion</a></li>
            </ul>
        </nav>
    </header>

    <main>
        <section id="introduction">
            <h2>Introduction</h2>

            <p>Liquid neural networks, a relatively new concept, represents a paradigm shift in how we design, train, and deploy computational models inspired by biological systems. Unlike traditional deep neural networks, which generally rely on fixed architectures and static parameters once trained, liquid neural networks—often referred to as liquid time-constant networks—are dynamic, continually adapting their internal parameters in response to changing inputs and conditions. It draws <strong>inspiration</strong> from how natural biological neurons operate in real neural circuits, where synaptic strengths and temporal patterns of neural firing evolve continuously, enabling brains to adapt to unpredictable environments.
            </p>
            
            <figure style="text-align: center;">
                <img src="brain.gif" alt="Descriptive text" style="width:100%; max-width:500px;">
                <figcaption style="margin-top: 5px; font-style: italic; color: gray;">
                  Fig1: An image of the brain with continuous neural firings
                </figcaption>
              </figure>

            <p>
            Conventional deep learning models have achieved remarkable success in tasks such as image classification, language translation, and speech recognition. However, their rigid architectures and reliance on large, static datasets often leave them ill-suited for scenarios where conditions fluctuate rapidly or data is scarce. For instance, robotic agents operating in uncertain environments need to learn and adapt on-the-fly  <sup><a href="#ref1" class="citation">[1]</a></sup>. Similarly, autonomous vehicles must dynamically respond to variations in traffic patterns, weather conditions, or road anomalies. Liquid neural networks provide a mechanism for continuous adaptation, allowing these systems to not only perform well in dynamic contexts but also to generalize effectively beyond their training conditions  <sup><a href="#ref2" class="citation">[2]</a></sup> .
            </p>
            
            <figure>
                <img src="traffic1.png" alt="A visualization of dynamical contexts">
                <figcaption> Fig 2: Visualization of dynamical contexts which can be addressed by a Liquid Neural Network</figcaption>
            </figure>

            <p>
            Research into liquid neural networks is still in its early stages, yet the initial results are promising. By merging ideas from computational neuroscience, dynamical systems theory, and machine learning, liquid neural networks hold the potential to push the boundaries of what artificial systems can achieve. 
            </p>

            <blockquote>
                "We are thrilled by the immense potential of liquid neural networks, as it lays the groundwork for solving problems that arise when training in one environment and deploying in a completely distinct environment without additional training. These flexible algorithms could one day aid in decision-making based on data streams that change over time, such as medical diagnosis and autonomous driving applications" - Daniela Rus, CSAIL director and the Andrew (1956) and Erna Viterbi Professor of Electrical Engineering and Computer Science at MIT.
            </blockquote>
<!--
            <figure>
                <img src="image1.jpg" alt="Visualization of a Liquid Neural Network">
                <figcaption>A conceptual visualization of state dynamics within a Liquid Neural Network.</figcaption>
            </figure>
        </section>
    -->
        <section id="background">
            <h2>Background</h2>

            <p>
            The conceptual roots of liquid neural networks can be traced back to a lineage of research focused on bridging continuous-time dynamical systems with artificial intelligence. Well before the term “liquid neural networks” entered the lexicon, computational neuroscientists and machine learning researchers were investigating biologically inspired models that adapt their internal parameters in real time. Early groundwork came from the study of continuous-time recurrent neural networks (CTRNNs) <sup><a href="#ref3" class="citation">[3]</a></sup>, which used differential equations to capture the evolving state of neurons over time, and liquid state machines (LSMs) <sup><a href="#ref4" class="citation">[4]</a></sup>, which employed biologically plausible spiking units and reservoir computing principles to process time-varying inputs. These foundational ideas provided the fertile conceptual soil from which the notion of liquid neural networks would emerge.
            </p>

            <p>
            In the years leading up to the formal introduction of liquid neural networks, there was a growing recognition that conventional artificial neural architectures—often reliant on discretized time steps and fixed parameters—could not fully replicate the flexibility and adaptability observed in natural brains. Researchers at MIT CSAIL and other institutions began to probe this limitation more deeply, asking: <i>Could we design networks that inherently adjust their dynamics as conditions change?</i> This question set the stage for the eventual invention of liquid time-constant networks, a primary instance of liquid neural networks.
            </p>

            <p>
            The key breakthrough came from a team led by Daniela Rus, Director of MIT CSAIL, and first author Ramin Hasani. Drawing upon their interdisciplinary backgrounds in robotics, dynamical systems, and computational neuroscience, the team devised a framework that introduced parameters governing neural dynamics as functions that continuously vary in time. They called these Liquid Time-Constant Neural Networks (LTCs)  <sup><a href="#ref2" class="citation">[2]</a></sup>. 
            </p>
           
            <p>LTCs consists of a novel architecture where neurons dynamically adjust their time-constants based on input and hidden states. This is inspired by the dynamics of non-spiking biological neurons, emphasizing adaptability and stability in learning systems.</p>


<section id="methods"> 
    <h2>Methods</h2>


<p>Here is a deep dive into the inner workings of an LTC, which is unfortunately, sparsely / not well covered in research papers. I wanted to get into the depths of what LTCs actually consist of, and why have they been constructed the way they are. I then show you important insights into its workings, through some clever design of experiments and analysis.</p>

<h3>Derivation of LTC Model</h3>

<p>Let us begin with the basic dynamics of a neural system represented by the state variable \(x(t)\), which evolves over time as a function of input \(I(t)\), time \(t\), and parameters \(\theta\). The most general form of such a system is an ODE given by:</p>
<div class="equation">
    \[\frac{dx(t)}{dt} = f(x(t), I(t), t, \theta)\]
    <span class="equation-number">(1)</span>
</div>

<p>However, this approach relies heavily on implicit nonlinearities in the neural network \(f\), which limits its ability to capture complex temporal patterns. To overcome this, LTC networks introduce dynamic time-constants that depend on the input \(I(t)\) and state \(x(t)\). This adjustment enables neurons to act as specialized dynamical systems for different input features, enhancing the system's expressivity.</p>

<h4>1. Adding a Linear Decay Term</h4>
<p>To ensure stability and bounded dynamics, a linear decay term proportional to the state \(x(t)\) is introduced. This modifies the equation to:</p>
<div class="equation">
    \[\frac{dx(t)}{dt} = -\frac{x(t)}{\tau} + f(x(t), I(t), t, \theta)\]
    <span class="equation-number">(2)</span>
</div>
<p>
Here:
<ul>
    <li>\(-\frac{x(t)}{\tau}\): Linear decay term, where \(\tau\) is the time constant.</li>
    <li>\(f(x(t), I(t), t, \theta)\): Nonlinear term capturing the influence of inputs and parameters.</li>
</ul>
</p>

<h4>2. Introducing a Modulating Nonlinearity</h4>
<p>We further enhance the system by defining \(f(x(t), I(t), t, \theta)\) as a product of a nonlinear function \(g\) and a term representing the equilibrium level \(A - x(t)\):</p>
<div class="equation">
    \[f(x(t), I(t), t, \theta) = g(x(t), I(t), t, \theta) \cdot (A - x(t))\]
    <span class="equation-number">(3)</span>
</div>
<p>Substituting this into the equation gives:</p>
<div class="equation">
    \[\frac{dx(t)}{dt} = -\frac{x(t)}{\tau} + g(x(t), I(t), t, \theta) \cdot (A - x(t))\]
    <span class="equation-number">(4)</span>
</div>

<h4>3. Expanding the Dynamics</h4>
<p>We expand the second term to separate the influence of \(x(t)\):</p>
<div class="equation">
    \[\frac{dx(t)}{dt} = -\frac{x(t)}{\tau} - g(x(t), I(t), t, \theta) \cdot x(t) + g(x(t), I(t), t, \theta) \cdot A\]
    <span class="equation-number">(5)</span>
</div>
<p>Combining terms involving \(x(t)\), we get:</p>
<div class="equation">
    \[\frac{dx(t)}{dt} = -\left[\frac{1}{\tau} + g(x(t), I(t), t, \theta)\right] \cdot x(t) + g(x(t), I(t), t, \theta) \cdot A\]
    <span class="equation-number">(6)</span>
</div>

<h4>4. Defining the Liquid Time Constant</h4>
<p>The effective time constant \(\tau_{\text{sys}}\) is defined as:</p>
<div class="equation">
    \[\frac{1}{\tau_{\text{sys}}} = \frac{1}{\tau} + g(x(t), I(t), t, \theta)\]
    <span class="equation-number">(7)</span>
</div>
<!-- 
<p>From this, we can express \(\tau_{\text{sys}}\) as:</p>
<div class="equation">
    \[\tau_{\text{sys}} = \frac{\tau}{1 + \tau \cdot g(x(t), I(t), t, \theta)}\]
    <span class="equation-number">(8)</span>
</div>
-->
<h4>5. Final Dynamics</h4>
<p>Hence, the final governing equation for the LTC Network is:</p>
<div class="equation">
    \[\frac{dx(t)}{dt} = -\frac{x(t)}{\tau_{\text{sys}}} + g(x(t), I(t), t, \theta) \cdot A\]
    <span class="equation-number">(9)</span>
</div>


<p> The neural network \(f\) not only determines the derivative of the hidden state \(x(t)\), but also serves as an input-dependent varying time-constant \(\tau_{\text{sys}} = \frac{\tau}{1+\tau \cdot f(x(t), I(t), t, \theta)}\) for the learning system. The time constant characterizes the speed and coupling sensitivity of an ODE. This property enables single elements of the hidden state to identify specialized dynamical systems for input features arriving at each time-point. Hence, these models were given the name: liquid time-constant recurrent neural networks (LTCs). LTCs can be implemented by an arbitrary choice of ODE solvers.</p>

<p>This formulation ensures that the dynamics of LTC networks are not only stable but also bounded, a critical requirement for real-world time-series modeling where inputs may grow unbounded.</p>

<table>
    <caption>Trainable Parameters in LTC Implementation</caption>
    <thead>
      <tr>
        <th class="serial-column">#</th>
        <th>Parameter</th>
        <th>Definition</th>
        <th>Role</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td class="serial-column">1</td>
        <td>gleak</td>
        <td>Leak conductance for each neuron.</td>
        <td>Controls how quickly the neuron’s state decays toward its resting potential.</td>
      </tr>
      <tr>
        <td class="serial-column">2</td>
        <td>vleak</td>
        <td>Leak reversal potential (resting state) for each neuron.</td>
        <td>Defines the baseline voltage to which the neuron decays in the absence of input.</td>
      </tr>
      <tr>
        <td class="serial-column">3</td>
        <td>cm</td>
        <td>Membrane capacitance for each neuron.</td>
        <td>Determines the neuron’s ability to store charge, affecting input integration.</td>
      </tr>
      <tr>
        <td class="serial-column">4</td>
        <td>sigma</td>
        <td>Scaling factor for sigmoid gating between neurons.</td>
        <td>Modulates the strength of synaptic activation based on neuron states.</td>
      </tr>
      <tr>
        <td class="serial-column">5</td>
        <td>mu</td>
        <td>Offset for the sigmoid gating function.</td>
        <td>Controls the sensitivity of neuron interactions to presynaptic signals.</td>
      </tr>
      <tr>
        <td class="serial-column">6</td>
        <td>w</td>
        <td>Synaptic weights between neurons.</td>
        <td>Defines the strength of recurrent connections.</td>
      </tr>
      <tr>
        <td class="serial-column">7</td>
        <td>erev</td>
        <td>Synaptic reversal potential.</td>
        <td>Sets the equilibrium potential that synaptic currents drive neurons toward.</td>
      </tr>
      <tr>
        <td class="serial-column">8</td>
        <td>sensory_sigma</td>
        <td>Scaling factor for sigmoid gating of sensory inputs.</td>
        <td>Modulates the sensitivity of neurons to external inputs.</td>
      </tr>
      <tr>
        <td class="serial-column">9</td>
        <td>sensory_mu</td>
        <td>Offset for sigmoid gating of sensory inputs.</td>
        <td>Shifts the input-response curve for sensory neurons.</td>
      </tr>
      <tr>
        <td class="serial-column">10</td>
        <td>sensory_w</td>
        <td>Weights for sensory inputs to neurons.</td>
        <td>Determines how much influence each input dimension has on the neurons.</td>
      </tr>
      <tr>
        <td class="serial-column">11</td>
        <td>sensory_erev</td>
        <td>Reversal potential for sensory input synapses.</td>
        <td>Sets the equilibrium potential for sensory-driven currents.</td>
      </tr>
      <tr>
        <td class="serial-column">12</td>
        <td>input_w</td>
        <td>Linear scaling weights for sensory inputs.</td>
        <td>Multiplies raw input values before applying additional transformations.</td>
      </tr>
      <tr>
        <td class="serial-column">13</td>
        <td>input_b</td>
        <td>Bias for sensory input mapping.</td>
        <td>Adds a constant offset to sensory inputs after scaling.</td>
      </tr>
      <tr>
        <td class="serial-column">14</td>
        <td>output_w</td>
        <td>Linear scaling weights for output neurons.</td>
        <td>Determines how neuron states are combined to produce the output.</td>
      </tr>
      <tr>
        <td class="serial-column">15</td>
        <td>output_b</td>
        <td>Bias for output mapping.</td>
        <td>Adds a constant offset to the output after scaling.</td>
      </tr>
    </tbody>
  </table>

  
      <h4>Parameter Categories</h4>
      <ul>
          <li><strong>Intrinsic Properties:</strong>
              <ul>
                  <li><code>gleak</code>: Leak conductance</li>
                  <li><code>vleak</code>: Leak reversal potential</li>
                  <li><code>cm</code>: Membrane capacitance</li>
              </ul>
          </li>
          <li><strong>Recurrent Dynamics:</strong>
              <ul>
                  <li><code>w</code>: Weight matrix of recurrent connections</li>
                  <li><code>sigma</code>: Activation standard deviation</li>
                  <li><code>mu</code>: Activation mean</li>
                  <li><code>erev</code>: Reversal potential for recurrent connections</li>
              </ul>
          </li>
          <li><strong>Sensory Inputs:</strong>
              <ul>
                  <li><code>sensory_w</code>: Weight matrix for sensory inputs</li>
                  <li><code>sensory_sigma</code>: Standard deviation of sensory activations</li>
                  <li><code>sensory_mu</code>: Mean of sensory activations</li>
                  <li><code>sensory_erev</code>: Reversal potential for sensory inputs</li>
              </ul>
          </li>
          <li><strong>External Inputs:</strong>
              <ul>
                  <li><code>input_w</code>: Weight matrix for external inputs</li>
                  <li><code>input_b</code>: Bias for external inputs</li>
              </ul>
          </li>
          <li><strong>Output Mapping:</strong>
              <ul>
                  <li><code>output_w</code>: Weight matrix for output mapping</li>
                  <li><code>output_b</code>: Bias for output mapping</li>
              </ul>
          </li>
      </ul>
  
  
      <h3>Quantifiable Relationships Between Parameters</h3>
        
        <div class="equation">
            \[ I_{recurrent} = w \cdot (\sigma \cdot activation(\mu, V)) \]
            <span class="equation-number">(a)</span>
        </div>
        <div class="equation">
            \[ I_{sensory} = sensory\_w \cdot (sensory\_\sigma \cdot activation(sensory\_\mu, V)) \]
            <span class="equation-number">(b)</span>
        </div>

        <div class="equation">
            \[ I_{input} = input\_w \cdot Input + input\_b \]
            <span class="equation-number">(c)</span>
        </div>
        <div class="equation">
            \[ Output = output\_w \cdot State + output\_b \]
            <span class="equation-number">(d)</span>
        </div>
            <div class="equation">
                \[ \frac{dV}{dt} = \frac{gleak \cdot (vleak - V) + I_{recurrent} + I_{sensory} + I_{input}}{cm} \]
                <span class="equation-number">(e)</span>
            </div>


</section>

        <section id="experiments">
            <h2>Experiments and Results</h2>


            <p> An experiment was designed to simulate time-series prediction task using synthetic data. The input feature consists of two signals: a sine wave and a cosine wave, both sampled over a length of \( N = 48 \). These input signals were generated using \( \sin \) and \( \cos \) functions over a range of \( 0 \) to \( 3\pi \). The target output is a sine wave with double the frequency of the input sine signal, generated over the same length.

                The input features were organized as a batch of size 1, resulting in a shape of \( (1, 48, 2) \), while the target output was reshaped to have a shape of \( (1, 48, 1) \). A visualization of the data reveals the relationship between the input signals and the target output, providing insight into the patterns the model is expected to learn.</p>

                <figure>
                    <img src="training.png" alt="Performance comparison across architectures">
                    <figcaption>Fig 3: Training Data</figcaption>
                </figure>
            
     
            
               <p> For this experiment, the model employed a fully connected wiring scheme with 8 units, one of which was designated as a motor neuron to produce the output signal. This configuration was implemented using the <i>FullyConnected</i> wiring class. The model architecture consisted of two main layers: an input layer to accept sequences of two input features (sine and cosine waves) and the LTC layer with the defined wiring. The output of the LTC layer was configured to return sequences to match the time-series format of the target data. </p>
                <figure>
                    <img src="wiring.png" alt="Performance comparison across architectures">
                    <figcaption>Fig 4: LTC model architecture</figcaption>
                </figure>
                
                <ul>
                    <li><strong>Neuron Types:</strong></li>
                    <ul>
                      <li><strong>Motor Neuron (1):</strong> This neuron generates the output of the model, acting as the final layer. It connects to all other neurons to compute the output.</li>
                      <li><strong>Interneurons (7):</strong> These neurons are recurrently connected and interact with each other. They capture temporal dependencies and dynamics within the LTC layer.</li>
                      <li><strong>Sensory Neurons (2):</strong> These neurons handle the input data, connecting the input features (sine and cosine waves) to the interneurons.</li>
                    </ul>
                  </ul>

<p>
The model was compiled using the Adam optimizer with a learning rate of 0.01 and a mean squared error loss function, which is suitable for regression tasks.  </p>


       
            <h3>Analysis</h3>

            <p>
               <strong>1.</strong>  Upon training the model, the recurrent weight matrix looks like:
            </p>
            <figure>
                <img src="recurrent.png" alt="Performance comparison across architectures">
                <figcaption>Fig 5: Recurrent weight matrix</figcaption>
            </figure>
            
            <p>This represents the strength of connections between the neurons in the recurrent network. Each element <code>w[i, j]</code> in the matrix defines the weight of the connection from neuron <code>j</code> to neuron <code>i</code>.</p>

            <p><strong>Matrix Interpretation:</strong></p>
            <ul>
              <li>Rows correspond to the receiving neurons.</li>
              <li>Columns correspond to the sending neurons.</li>
              <li>The value of each element <code>w[i, j]</code>:
                <ul>
                  <li>Positive values indicate excitatory connections (enhancing the activity of the receiving neuron).</li>
                  <li>Zero or near-zero values imply weak or no connection between the neurons.</li>
                </ul>
              </li>
            </ul>
            
            <p>This visualization highlights how the network neurons interact and provides insights into the model's temporal dynamics.</p>


            <p><strong>2.</strong> Next, we look at the impact of varying the leak conductance parameter (<code>gleak</code>) on the neuron dynamics in the LTC model. By adjusting <code>gleak</code> to different values (<code>0.1</code>, <code>0.5</code>, and <code>1.0</code>) and observing the model's predictions, the analysis highlights how this parameter influences the temporal behavior of the network.</p>


            <figure>
                <img src="gleak.png" alt="Performance comparison across architectures">
                <figcaption>Fig 6: Varying gleak values</figcaption>
            </figure>
            

<p>The results are visualized in a plot where the state values over time are compared for each <code>gleak</code> value. Lower <code>gleak</code> values result in slower decay or prolonged activity, while higher values lead to faster decay and reduced persistence of neuron states. </p>





<p><strong>3. </strong>The next experiment explores the effect of perturbing sensory parameters in the LTC model on its output dynamics. Three key sensory parameters were modified:</p>
<ul>
  <li><code>sensory_w</code>: The sensory weights, scaled by factor of 2.0.</li>
  <li><code>sensory_mu</code>: The sensory mean, shifted by adding 0.2.</li>
  <li><code>sensory_sigma</code>: The sensory standard deviation, broadened by a factor of 1.5.</li>
</ul>

<figure>
    <img src="sens_pert.png" alt="Performance comparison across architectures">
    <figcaption>Fig 7: Sensory Parameter Perturbations</figcaption>
</figure>

<p><strong>1. Scaling <code>sensory_w</code> (Sensory Weights):</strong></p>
<p>These weights determine how strongly the input features influence the neurons. Scaling them affects the magnitude of the contribution from sensory inputs, thereby altering the overall network output. Smaller weights reduce the input's influence, while larger weights amplify it.</p>

<p><strong>2. Shifting <code>sensory_mu</code> (Sensory Mean):</strong></p>
<p>The mean (<code>mu</code>) represents the baseline or threshold for input activation. Shifting <code>mu</code> changes the input's bias, altering when and how the neurons respond to the inputs. For example, increasing <code>mu</code> requires larger input signals to achieve the same level of neuron activation, hence modifying the activation threshold.</p>

<p><strong>3. Broadening <code>sensory_sigma</code> (Sensory Standard Deviation):</strong></p>
<p>The standard deviation (<code>sigma</code>) controls the sensitivity to input variability. Broadening <code>sigma</code> (e.g., scaling by 1.5) increases the range over which inputs influence the neuron, potentially smoothing the response and making the network less sensitive to small changes in input.</p>

<p>These changes illustrate the interpretability and flexibility of the LTC model. The perturbations allow us to observe how specific parameter adjustments influence the temporal behavior and outputs of the network.</p>


<p><strong>4. </strong> The next experiment conducts a global sensitivity analysis to evaluate the impact of perturbing individual trainable parameters in the LTC layer on the model's output. By applying small random perturbations to each parameter and measuring the resulting change in the model's predictions, the sensitivity of each parameter is quantified.</p>

<figure>
    <img src="sens.png" alt="Performance comparison across architectures">
    <figcaption>Fig 8: Global Sensitivity Analysis</figcaption>
</figure>


<p>The sensitivity score for a parameter is computed as the mean absolute difference between the model's output with the perturbed parameter and the original output. This analysis provides insight into which parameters have the most significant influence on the model's behavior, helping to identify key components of the LTC dynamics.</p>


<p><strong>5. </strong> Finally, I explore the relationship between the trained parameters in the LTC layer by examining their pairwise correlations. The parameters include both neuron-specific values (e.g., <code>gleak</code>, <code>vleak</code>, <code>cm</code>) and connection-specific values (e.g., <code>w</code>, <code>sigma</code>, <code>mu</code>, and sensory parameters). Each parameter is extracted, flattened, and padded to ensure consistent lengths for correlation computation.</p>

<p>A correlation matrix is calculated to capture the degree of linear association between all pairs of parameters. This matrix provides insights into how changes in one parameter might relate to changes in another, highlighting dependencies or redundancies in the model's learned dynamics.</p>

<p>The correlation matrix is visualized as a heatmap, with parameter names labeled along the axes. Colors indicate the strength and direction of correlations:
<ul>
  <li>Red tones represent strong positive correlations.</li>
  <li>Blue tones indicate strong negative correlations.</li>
  <li>White or neutral tones signify little to no correlation.</li>
</ul>
This visualization provides a clear and interpretable overview of parameter interactions within the LTC layer.</p>

            <figure>
                <img src="corr2.png" alt="Performance comparison across architectures">
                <figcaption>Fig 9: Correlation heatmap between the parameters</figcaption>
            </figure>


        </section>

        <section id="discussion">
            <h2>Discussion</h2>
            <p>The experiments conducted using the Liquid Time-Constant (LTC) model offer us valuable insights into its behavior, interpretability, and effectiveness in capturing temporal dynamics in a time-series prediction task. The use of synthetic data, combined with targeted parameter modifications, highlights the model's ability to adapt to complex temporal relationships and provides a framework for understanding its internal mechanisms.</p>
        
            <p>Through the visualization of the recurrent weight matrix, we observe how the strength of connections between neurons facilitates the propagation of temporal information. The analysis of neuron dynamics with varying <code>gleak</code> values demonstrates how the leak conductance regulates signal decay, influencing the stability and responsiveness of the network. Moreover, the perturbation of sensory parameters (<code>sensory_w</code>, <code>sensory_mu</code>, <code>sensory_sigma</code>) reveals the flexibility of the LTC model in adjusting its sensitivity to input features. The ability to scale, shift, and broaden the influence of input signals underscores the adaptability of the model and its potential for handling diverse data patterns. These experiments validate the LTC's capability to integrate input signals in a controlled and interpretable manner.</p>
        
            <p>The global sensitivity analysis further emphasizes the importance of individual parameters, identifying key contributors to the model's predictive behavior. This systematic evaluation helps prioritize parameters for fine-tuning and offers insights into the LTC's inner workings. Similarly, the exploration of parameter correlations provides a broader understanding of interdependencies within the model, revealing potential redundancies or synergies that could inform future model refinements.</p>
        
            <p>Overall, the experiments demonstrate the LTC model's robustness and interpretability. The ability to analyze its parameters and their interactions enables a deeper understanding of its temporal dynamics, making it a powerful tool for time-series prediction tasks. These observations also align with the model's design to emulate biologically inspired dynamics. Future work could extend these findings to complex real-world datasets and explore more complex wiring configurations to further enhance the model's applicability.</p>
        

            <p>Few other observations based on experiments I ran, but didn't include in the blog:</p>

            <p>On simple synthetically derived datasets, vanilla RNNs do as well as LTCs, as there isn't much complexity. However, on more complex datasets, LTCs outperform RNNs<cite></cite>. This is because LTCs can adapt their time-constants to the data, while RNNs cannot. This is a key property that makes them more powerful than vanilla RNNs.

            Some of the other important advantages of LTCs that I could conclude from my experiements and literature were:
<ul>
    <li><strong>Bounded Dynamics:</strong> Ensures the state \(x(t)\) remains finite.</li>
    <li><strong>Adaptivity:</strong> The time constant \(\tau_{\text{sys}}\) dynamically adjusts based on input and state.</li>
    <li><strong>Expressivity:</strong> The nonlinear modulation enables modeling of complex temporal patterns.</li>
</ul>

<h3> Limitations</h3>

<p>While Liquid Time-Constant (LTC) models show significant promise in capturing temporal dynamics and performing well across various time-series prediction tasks, they also come with notable limitations that warrant further investigation:</p>

<p><strong>1. Long-Term Dependencies:</strong> Like many other time-continuous models, LTCs face challenges in capturing long-term dependencies due to the vanishing gradient problem during gradient descent training. This issue, highlighted in studies by Pascanu et. al. (2013)  <sup><a href="#ref5" class="citation">[5]</a></sup>, limits their applicability in tasks that require learning patterns over extended temporal sequences.</p>

<p><strong>2. Choice of ODE Solver:</strong> The performance of LTCs is heavily influenced by the choice of numerical implementation methods for solving the underlying ordinary differential equations (ODEs). While advanced variable-step solvers enable strong performance, using simpler methods like the explicit Euler solver can significantly impact their efficiency and accuracy. This dependency on solver choice introduces variability in their effectiveness.</p>

<p><strong>3. Time and Memory Complexity:</strong> LTCs, while highly expressive, incur substantial time and memory costs during training and inference. In comparison, Neural ODEs<sup><a href="#ref6" class="citation">[6]</a></sup>are faster but lack the expressive power of LTCs. The trade-off between computational complexity and model expressiveness highlights the need for further optimization of LTC architectures to make them more resource-efficient while retaining their powerful representational capabilities.</p>

        </section>

         <!-- References Section -->
        <section class="references">
            <h2>References</h2>
            <ol>

                <li id="ref1"> Chahine, Makram, et al. "Robust flight navigation out of distribution with liquid neural networks." Science Robotics 8.77 (2023): eadc8892.
                    <a href="https://www.science.org/doi/full/10.1126/scirobotics.adc8892" target="_blank">https://www.science.org/doi/full/10.1126/scirobotics.adc8892</a>
                </li>
                
                <li id="ref2">  Hasani, Ramin, et al. "Liquid time-constant networks." Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 35. No. 9. 2021.
                    <a href="https://ojs.aaai.org/index.php/AAAI/article/view/16936" target="_blank">https://ojs.aaai.org/index.php/AAAI/article/view/16936</a>
                </li>
               
                <li id="ref3">Beer, R. D. (1995). “On the dynamics of small continuous-time recurrent neural networks.” Adaptive Behavior, 3(4), 469–505
                  <a href="https://doi.org/10.1177/105971239500300405" target="_blank">https://doi.org/10.1177/105971239500300405</a></li>

                  <li id="ref4">Maass, Wolfgang. "Liquid state machines: motivation, theory, and applications." Computability in context: computation and logic in the real world (2011): 275-296.
                    <a href="https://www.worldscientific.com/doi/abs/10.1142/9781848162778_0008" target="_blank">https://www.worldscientific.com/doi/abs/10.1142/9781848162778_0008</a></li>
                  
                    <li id="ref5">Pascanu, R. "On the difficulty of training recurrent neural networks." arXiv preprint arXiv:1211.5063 (2013).
                        <a href="https://proceedings.mlr.press/v28/pascanu13.html" target="_blank">https://proceedings.mlr.press/v28/pascanu13.html</a></li>
                      
                        <li id="ref6">
                            Chen, Ricky TQ, et al. "Neural ordinary differential equations." Advances in neural information processing systems 31 (2018).
                            <a href="https://arxiv.org/abs/1806.07366" target="_blank">https://arxiv.org/abs/1806.07366</a></li>
                          

                  
            </ol>
        </section>
    </main>


    <footer>
        <p>&copy; 2024 Nidhish Sagar. nidhishs@mit.edu </p>
    </footer>

    <script>
        // Ensure the page starts at the top
        window.onload = function () {
        if (!window.location.href.includes('?')) {
            window.location.href += '?resetScroll';
        } else {
            window.scrollTo(0, 0);
        }
        };
        // Header scroll effect
        const header = document.getElementById('page-header');
        window.addEventListener('scroll', () => {
            if (window.scrollY > 100) {
                header.classList.add('header-scrolled');
            } else {
                header.classList.remove('header-scrolled');
            }
        });
    
        // Word count functionality
        document.addEventListener('DOMContentLoaded', function () {
            // Select the element(s) you want to count words from
            const main = document.querySelector('main');
            
            if (main) { // Ensure 'main' exists before accessing its content
                // Get the text content of the main element
                const text = main.textContent || main.innerText;
    
                // Trim and split the text by whitespace to count words
                const words = text.trim().split(/\s+/).filter(word => word.length > 0);
                const wordCount = words.length;
    
                // Display the word count
                const display = document.getElementById('wordCountDisplay');
                if (display) {
                    display.textContent = `Word Count: ${wordCount}`;
                }
            }
        });
    </script>
    
</body>
</html>
